---
title: "Matematyczne modelowanie procesów decyzyjnych w finansach"
author: "Rafał Głodek, Joanna Kusy, Oliwia Makuch, Tomasz Srebniak"
execute: 
  echo: true
format:
  revealjs:
    theme: serif
    transition: slide
    scrollable: true
    self-contained: true
    fontsize: "24pt"
title-slide-attributes:
    data-background-image: "images/img_prezentacja2.jpg"
    data-background-size: cover
    data-background-opacity: "0.65"
    data-background-color: "#000000"
---

## Powtórzenie

```{=html}
<style>
  .superbigimage{
      overflow-x:scroll;
      white-space: nowrap;
  }

  .superbigimage img{
     max-width: none;
  }


</style>
```

Model GARCH wykorzystywany jest przy modelowaniu i przewidywaniu zmienności w szeregach czasowych. Stosuje się go głównie w analizie finansowej.\

Wzór ogólny

$$
GARCH(p, q): \sigma_t^2=\omega+\sum_{i=1}^{p}{\alpha_i\epsilon_{t-i}^2} + \sum_{j=1}^{q}{\beta_j\sigma_{t-j}^2}
$$

Najczęściej omawiany przypadek

$$
GARCH(1, 1): \sigma_t^2=\omega+{\alpha\epsilon_{t-1}^2} + {\beta\sigma_{t-1}^2}
$$

$\omega$ - bazowy poziom wariancji warunkowej

$\alpha$ – współczynnik reakcji na nowe informacje (efekt ARCH)

$\beta$ – współczynnik pamięci zmienności (efekt GARCH)

$\epsilon_{t-1}^2$ – kwadrat błędu z kroku $t-1$

$\sigma_{t-1}^2$ – wariancja z kroku $t-1$

```{r setup}
#| echo: false
#| warning: false
#| error: false
#| output: false
library(reticulate)
options(reticulate.python = "/opt/homebrew/bin/python3")
```

```{python}
#| echo: false
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from arch import arch_model
from arch.univariate import SkewStudent
import seaborn as sns
from scipy import stats
```

```{python, echo=FALSE, output=FALSE}
df = pd.read_csv('aapl.us.txt')
df['Date'] = pd.to_datetime(df['Date'])
df.set_index('Date', inplace=True)
# Liczenie zwrotów procentowych
returns = df['Close'].pct_change().dropna()

normal_gm = arch_model(returns, p = 1, q = 1, 
mean = 'constant', 
vol = 'GARCH', 
dist = 'normal')

gm_result = normal_gm.fit()
```

## Założenia dotyczące rozkładu standaryzowanych reszt

$$
\text{reszta} = \epsilon_t = \text{predyktowany zwrot} - \text{średni zwrot}
$$

$$
\text{standaryzowana reszta} = \frac{\epsilon_t}{\sigma_t}
$$

```{python}
gm_resid = gm_result.resid
gm_std = gm_result.conditional_volatility

gm_std_resid = gm_resid / gm_std
```

\

-   Rozkład normalny (domyślna opcja): `"normal"`

-   Rozkład t-Studenta -- grube ogony rozkładu: `"t"`

-   Skośny rozkład t-Studenta -- grube ogony i skośność: `"skewt"`

Implementacja

```{python}
normal_gm = arch_model(
  returns, p = 1, q = 1, 
  mean = 'constant', 
  vol = 'GARCH', 
  dist = 'normal'  # założenie o rozkładzie
  )
```

```{python, echo=FALSE, output=FALSE}
normal_resid = np.random.normal(0, 1, len(gm_std_resid))

# tstudent
t_gm = arch_model(returns, p = 1, q = 1, 
mean = 'constant', 
vol = 'GARCH', 
dist = 't')

gm_t_result = t_gm.fit()

gm_t_resid = gm_t_result.resid
gm_t_std = gm_t_result.conditional_volatility

gm_t_std_resid = gm_t_resid /gm_t_std

df_hat = gm_t_result.params['nu']
t_resid = np.random.standard_t(df=df_hat, size=len(gm_t_std_resid))
t_resid_scaled = t_resid / np.sqrt(df_hat / (df_hat - 2))

# skośny tstudent
skewt_gm = arch_model(returns, p = 1, q = 1, 
mean = 'constant', 
vol = 'GARCH', 
dist = 'skewt')

gm_skewt_result = skewt_gm.fit()

gm_skewt_resid = gm_skewt_result.resid
gm_skewt_std = gm_skewt_result.conditional_volatility

gm_skewt_std_resid = gm_skewt_resid /gm_skewt_std

df_hat_skewt = gm_skewt_result.params['eta']
skew_hat = gm_skewt_result.params['lambda']

skewt = SkewStudent()
f_skew_resid = skewt.simulate((df_hat_skewt, skew_hat))
skewt_resid = f_skew_resid(len(gm_skewt_std_resid))
```

------------------------------------------------------------------------

### Rozkład normalny standaryzowanych reszt

```{python, echo=FALSE, output=TRUE}
#| fig-align: "center"
# plt.hist(gm_std_resid, bins = 50, 
#          facecolor = 'orange', label = 'Standardized residuals')
# plt.hist(normal_resid, bins = 50, 
#          facecolor = 'tomato', label = 'Normal residuals', alpha=0.7)
sns.histplot(gm_std_resid, stat='density', label='Standardized residuals')
# sns.kdeplot(normal_resid, color='red', label='Normal residuals', linestyle='--')
x = np.linspace(min(gm_std_resid), max(gm_std_resid), len(gm_std_resid))
plt.plot(x, stats.norm.pdf(x, 0, 1), color='red', label='Normal residuals', linestyle='--')
plt.legend(loc = 'upper left')
plt.show()
```

### Rozkład t--Studenta standaryzowanych reszt

```{python, echo=FALSE, output=TRUE}
#| fig-align: "center"
# plt.hist(gm_t_std_resid, bins = 50, 
#          facecolor = 'orange', label = 'Standardized residuals')
# plt.hist(t_resid_scaled, bins = 50, 
#          facecolor = 'tomato', label = 't-Student residuals', alpha=0.7)
sns.histplot(gm_t_std_resid, stat='density', label='Standardized residuals')
# sns.kdeplot(t_resid_scaled, color='red', label='t-Student residuals', linestyle='--')
x = np.linspace(min(gm_t_std_resid), max(gm_t_std_resid), len(gm_t_std_resid))
plt.plot(x, stats.t.pdf(x, df_hat), color='red', label='t-Student residuals', linestyle='--')
plt.legend(loc = 'upper left')
plt.show()
```

### Skośny rozkład t--Studenta standaryzowanych reszt

```{python , echo=FALSE}
import numpy as np
from scipy.special import gamma

def compute_constants(eta, lam):
    assert 2 < eta < np.inf, "eta must be greater than 2"
    assert -1 < lam < 1, "lambda must be in (-1, 1)"

    c = gamma((eta + 1) / 2) / (np.sqrt(np.pi * (eta - 2)) * gamma(eta / 2))
    a = 4 * lam * c * ((eta - 2) / (eta - 1))
    b_squared = 1 + 3 * lam**2 - a**2
    b = np.sqrt(b_squared)
    
    return a, b, c

def g(z, eta, lam):
    a, b, c = compute_constants(eta, lam)
    
    # Convert z to a numpy array for element-wise operations
    z = np.asarray(z)
    
    # Piecewise definition
    left_mask = z < -a / b
    right_mask = ~left_mask
    
    result = np.empty_like(z, dtype=float)
    
    # Left side
    left_term = 1 + (1 / (eta - 2)) * ((b * z[left_mask] + a) / (1 - lam))**2
    result[left_mask] = b * c * left_term**(-(eta + 1)/2)
    
    # Right side
    right_term = 1 + (1 / (eta - 2)) * ((b * z[right_mask] + a) / (1 + lam))**2
    result[right_mask] = b * c * right_term**(-(eta + 1)/2)
    
    return result
```

```{python, echo=FALSE, output=TRUE}
#| fig-align: "center"
# plt.hist(gm_skewt_std_resid, bins = 50, 
#          facecolor = 'orange', label = 'Standardized residuals')
# plt.hist(skewt_resid, bins = 50, 
#          facecolor = 'tomato', label = 'skewt residuals', alpha=0.7)
sns.histplot(gm_skewt_std_resid, stat='density', label='Standardized residuals')
# sns.kdeplot(skewt_resid, color='red', label='skewt residuals', linestyle='--')
x = np.linspace(min(gm_skewt_std_resid), max(gm_skewt_std_resid), len(gm_skewt_std_resid))
plt.plot(x, g(x, df_hat_skewt, skew_hat), color='red', label='skewt residuals', linestyle='--')
plt.legend(loc = 'upper left')
plt.show()
```

## Założenia o średniej

-   Stała średnia (domyślna opcja): `"constant"`

-   Zerowa średnia: `"zero"`

-   Średnia modelowana jako proces AR: `"AR"` (np. AR(1), AR(2), ...)

Implementacja

```{python}
ar_gm = arch_model(
  returns, p = 1, q = 1, 
  mean = 'AR', lags = 1,  # założenie o średniej
  vol = 'GARCH', 
  dist = 'normal' 
  )
```

```{python, echo=FALSE, output=FALSE}
constant_gm = arch_model(
  returns, p = 1, q = 1,
  mean = 'constant',
  vol = 'GARCH',
  dist = 'normal'
  )
zero_gm = arch_model(
  returns, p = 1, q = 1,
  mean = 'zero',
  vol = 'GARCH',
  dist = 'normal'
  )
cmean_result = constant_gm.fit()
armean_result = ar_gm.fit()
zero_result = zero_gm.fit()

cmean_vol = cmean_result.conditional_volatility
armean_vol = armean_result.conditional_volatility
zeromean_vol = zero_result.conditional_volatility
```

## Wpływ założeń o średniej

... na szacowaną zmienność warunkową

```{python echo=FALSE, output=TRUE}
#| fig-align: "center"
plt.plot(cmean_vol, color = 'blue', label = 'Constant Mean Volatility')
plt.plot(armean_vol, color = 'red', label = 'AR Mean Volatility', alpha=0.7)
plt.plot(zeromean_vol, color = 'green', label = 'Zero Mean Volatility', alpha=0.7, linestyle='dashed')
plt.legend(loc = 'upper right')
plt.xlabel('')
plt.savefig('mean_volatility.png')
```

Korelacja między wynikami

```{python echo=FALSE, output=TRUE}
# Łączymy w macierz (3 x n)
all_vols = np.vstack([cmean_vol, armean_vol, zeromean_vol])

# Usuwamy kolumny zawierające NaN
valid = ~np.isnan(all_vols).any(axis=0)

# Liczymy korelację tylko na poprawnych danych
corr = np.corrcoef(all_vols[:, valid])
corr_df = pd.DataFrame(corr, 
                       columns=['Constant Mean', 'AR Mean', 'Zero Mean'], 
                       index=['Constant Mean', 'AR Mean', 'Zero Mean'])
corr_df.style.set_table_styles(
    [{'selector': 'th', 'props': [('text-align', 'center'), ('font-size', '23px')]},
     {'selector': 'td', 'props': [('text-align', 'center'), ('font-size', '23px')]}]
).set_properties(**{
    'margin-left': 'auto',
    'margin-right': 'auto',
    'font-size': '10px',
    'background-color': 'transparent',
    'color': 'black'  # lub 'white', jeśli tło slajdu jest ciemne
})

```

## Asymetryczne wahania zmienności

Model GARCH zakłada, że zmienność reaguje symetrycznie na zdarzenia rynkowe. W praktyce jednak negatywne informacje (np. spadki cen) wywołują silniejszy wzrost zmiennościniż pozytywne.

![](images/symmetry_vs_asymmetry.png){fig-align="center"}

## Modele dla asymetrycznych wahań zmienności

### GJR--GARCH(p, o, q)

-   umożliwia większy wpływ negatywnych szoków na zmienność poprzez dodatkowy warunkowy składnik $(\gamma > 0)$

    $$
    \sigma^2 = \omega + \sum_{i=1}^p\alpha_i\epsilon_{t-i}^2+\sum_{j=1}^o\gamma_jI_{\{\epsilon_{t-j}<0\}}\epsilon_{t-j}^2+\sum_{k=1}^q\beta_k\sigma_{t-k}^2
    $$

Implementacja

```{python, eval=FALSE}
arch_model(
  returns, p = 1, q = 1, 
  o = 1,  # ustawiamy rząd składnika asymetrycznego
  mean = 'constant', 
  vol = 'GARCH'
  )
```

\

### EGARCH(p, o, q)

$$
\ln\sigma_{t}^{2}=\omega+\sum_{i=1}^{p}\alpha_{i}\left(\left|e_{t-i}\right|-\sqrt{2/\pi}\right)+\sum_{j=1}^{o}\gamma_{j} e_{t-j}+\sum_{k=1}^{q}\beta_{k}\ln\sigma_{t-k}^{2},
$$

gdzie $e_{t}=\epsilon_{t}/\sigma_{t}$.

-   wykładniczy GARCH

-   parametry występujące w modelu nie muszą być dodatnie

Implementacja

```{python, eval=FALSE}
arch_model(
  returns, p = 1, q = 1, 
  o = 1, 
  mean = 'constant', 
  vol = 'EGARCH'  # określamy model
  )
```

## Porównanie modeli

```{python, echo=FALSE, message=FALSE}
# Specify GJR-GARCH model assumptions
gjr_gm = arch_model(returns, p = 1, q = 1, o = 1, vol = 'GARCH', dist = 'normal')

# Fit the model
gjrgm_result = gjr_gm.fit(disp = 'off')


# Specify EGARCH model assumptions
egarch_gm = arch_model(returns, p = 1, q = 1, o = 1, vol = 'EGARCH', dist = 'normal')

# Fit the model
egarch_result = egarch_gm.fit(disp = 'off')

gjrgm_vol = gjrgm_result.conditional_volatility
egarch_vol = egarch_result.conditional_volatility


plt.plot(returns.index, returns, color='grey', alpha=0.4, label='Price Returns')
plt.plot(returns.index, gm_std, color='blue', label='GARCH Volatility')
plt.plot(returns.index, gjrgm_vol, color='green', label='GJR-GARCH Volatility')
plt.plot(returns.index, egarch_vol, color='red', label='EGARCH Volatility')


_ = plt.xticks(rotation=45)
plt.gca().xaxis.set_major_locator(plt.MaxNLocator(10))  # max 10 dat
_ = plt.xlim(min(returns.index), max(returns.index))
plt.tight_layout()
plt.legend(loc='upper right');
plt.show()
```

### Zbliżenie na lata 1997--1998

```{python, echo=FALSE, message=FALSE}
# Zbliżenie na lata 1998–2002
plt.plot(returns.index, returns, color='grey', alpha=0.4, label='Price Returns');
plt.plot(returns.index, gm_std, color='blue', label='GARCH Volatility');
plt.plot(returns.index, gjrgm_vol, color='green', label='GJR-GARCH Volatility');
plt.plot(returns.index, egarch_vol, color='red', label='EGARCH Volatility');

plt.legend(loc='upper right');
_ = plt.xticks(rotation=45)

# Ustaw zakres dat dla przybliżenia
_ = plt.xlim(pd.Timestamp("1997-01-01"), pd.Timestamp("1998-12-31"))
_ = plt.ylim(-0.05, 0.2)

# Ustaw mniejszą liczbę etykiet
_ = plt.gca().xaxis.set_major_locator(plt.MaxNLocator(6))
plt.tight_layout()
plt.show()
```

## Prognozy oparte na oknie ruchomym

::: {style="margin: 50px auto; font-size:30pt;"}
... polegają na ciągłym dopasowywaniu i predykowaniu (1. step forward) przyszłości z uwzględnieniem nowych obserwacji.

-   Brak nachodzenia danych używanych do predykcji i tych, które są predykowane

-   Model jest mniej podatny na nadmierne dopasowanie (overfitting)

-   Lepsze dostosowanie do bieżącej sytuacji rynkowej
:::

------------------------------------------------------------------------

### Expanding window forecast

... czyli poszerzające się okno ruchome

![](images/expanding_window_forecast.png){fig-align="center" width="700"}

### Fixed window forecast

... czyli stałe okno ruchome

![](images/fixed_window_forecast.png){fig-align="center" width="700"}

### Jak ustalić rozmiar okna?

-   Zwykle ustalane indywidualnie dla każdego przypadku.

-   **Zbyt szeroki rozmiar okna:** uwzględnienie nieaktualnych danych, które mogą prowadzić do wysokiego błędu.

-   **Zbyt wąski rozmiar okna**: wykluczenie istotnych danych, co może prowadzić do wyższej wariancji.

-   **Optymalny rozmiar okna**: kompromis równoważący odchylenie i wariancję.

## Rolling window forecast -- implementacja

::: {.scrollable}
```{python}
date = '1997-07-08'
start_loc = returns.index.get_loc(date)
end_loc = start_loc + 500
forecasts = {}
model = arch_model(returns, vol='GARCH', p=1, q=1, dist='normal')

for i in range(70):
    gm_result = model.fit(first_obs = i + start_loc, 
                             last_obs = i + end_loc,
                             disp='off')
    temp_result = gm_result.forecast(horizon = 1).variance
    last_index = returns.index[i + end_loc - 1]  
    fcast = temp_result.loc[last_index]  
    forecasts[fcast.name] = fcast
    
variance_fixedwin = pd.DataFrame(forecasts).T
vol_fixedwin = np.sqrt(variance_fixedwin)
```

```{python, echo=FALSE, output=FALSE, result:"hide"}
start_loc = returns.index.get_loc(date)
end_loc = start_loc + 500
forecasts = {}
model = arch_model(returns, vol='GARCH', p=1, q=1, o=1, dist='normal')
for i in range(70):
    gm_result = model.fit(first_obs = start_loc, 
                             last_obs = i + end_loc, disp='off')
    temp_result = gm_result.forecast(horizon = 1).variance
    last_index = returns.index[i + end_loc - 1]  
    fcast = temp_result.loc[last_index]  
    forecasts[fcast.name] = fcast
variance_expandwin = pd.DataFrame(forecasts).T

# Calculate volatility from variance forecast with an expanding window
vol_expandwin = np.sqrt(variance_expandwin)


# Plot results
plt.figure(figsize=(16,8));

# Plot volatility forecast with an expanding window
plt.plot(vol_expandwin, color = 'blue', label='Expanding Window');

# Plot volatility forecast with a fixed rolling window
plt.plot(vol_fixedwin, color = 'red', label='Rolling Window');

plt.plot(returns.loc[variance_expandwin.index], color = 'grey', label='Daily Return');

plt.legend();
plt.savefig("rolling_window_forecast.png");
```
\

### Porównanie podejść
![](rolling_window_forecast.png){style="display: block; margin-left: auto; margin-right: auto; width: 1000px;"}
:::
## ARMA-GARCH w pakiecie R

```{r}
#| echo: TRUE

library(xts)
library(zoo)
library(rugarch)

aapl <- read.csv('aapl.us.txt', header = TRUE, sep = ",")
aapl_xts <- xts(aapl[, 5], order.by = as.Date(aapl[, 1]))
returns <- diff(log(aapl_xts))
returns <- na.omit(returns)

spec <- ugarchspec(mean.model = list(armaOrder = c(1, 1), include.mean = TRUE),
              variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
              distribution.model = "norm")
fit <- ugarchfit(spec, returns)
print(fit)
```

## Źródła

-   https://bookdown.org/compfinezbook/introcompfinr/maximum-likelihood-estimation.html

-   QUASI-MAXIMUM LIKELIHOOD ESTIMATION AND INFERENCE IN DYNAMIC MODELS WITH TIME-VARYING COVARIANCES https://public.econ.duke.edu/\~boller/Published_Papers/ectrev_92.pdf

-   https://users.ssc.wisc.edu/\~bhansen/papers/ier_94.pdf

-   https://arch.readthedocs.io

-   https://goldinlocks.github.io/ARCH_GARCH-Volatility-Forecasting/
